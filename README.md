# 多模态学习资料

<img src="mm.png" alt="image-20220414152739544" style="zoom: 80%;" />

## 1 综述

- [Multimodal Machine Learning: A Survey and Taxonomy](https://arxiv.org/abs/1705.09406) , 2018. 多模态学习综述
- [A Survey on Multi-view Learning](https://arxiv.org/abs/1304.5634) , 2013. 多视图学习综述
- [Multimodal Intelligence: Representation Learning, Information Fusion, and Applications](https://arxiv.org/abs/1911.03977) , 2019 . 多模态表示和融合综述
- [An Overview of Cross-media Retrieval: Concepts, Methodologies, Benchmarks and Challenges](https://arxiv.org/abs/1704.02223) , 2017. 跨模态检索综述
- [Recent Advances and Trends in Multimodal Deep Learning: A Review](https://arxiv.org/abs/2105.11087) , 2021. 多模态深度学习综述
- [Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods](https://arxiv.org/abs/1907.09358) , 2021. 多模态学习应用综述
- [Transformers in Vision: A Survey](https://arxiv.org/abs/2101.01169) , 2021. 视觉Transformer综述
- [VLP: A Survey on Vision-Language Pre-training](https://arxiv.org/abs/2202.09061) , 2022. 多模态Transformer综述

## 2 模态联合学习

- [Tensor Fusion Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/1707.07250) , EMNLP, 2017.
- [Efficient Low-rank Multimodal Fusion with Modality-Specific Factors](https://arxiv.org/abs/1806.00064) , ACL, 2018.
- [Comprehensive Semi-Supervised Multi-Modal Learning](https://www.ijcai.org/proceedings/2019/568) , IJCAI, 2019.
- [Multi-Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis](https://ojs.aaai.org/index.php/AAAI/article/view/3807) , AAAI, 2019.
- [What Makes Training Multi-modal Classification Networks Hard](https://arxiv.org/abs/1905.12681) , CVPR, 2020.
- [Trusted Multi-View Classification](https://arxiv.org/abs/2102.02051) , ICLR, 2021.
- [Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis](https://arxiv.org/abs/2109.00412) , EMNLP, 2021.

## 3 跨模态学习

### 3.1 模态对齐

- [VSE++: Improving Visual-Semantic Embeddings with Hard Negatives](https://arxiv.org/abs/1707.05612) , BMVC, 2018.
- [Stacked Cross Attention for Image-Text Matching](https://arxiv.org/abs/1803.08024) , ECCV, 2018.
- [Position Focused Attention Network for Image-Text Matching](https://arxiv.org/abs/1907.09748) , IJCAI, 2019.
- [IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval](https://arxiv.org/abs/2003.03772) , CVPR, 2020.
- [Similarity Reasoning and Filtration for Image-Text Matching](https://arxiv.org/abs/2101.01368) , AAAI, 2021.
- [Rethinking Label-Wise Cross-Modal Retrieval from A Semantic Sharing Perspective](https://www.ijcai.org/proceedings/2021/0454.pdf) , IJCAI, 2021.

### 3.2 模态翻译

- [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555) , CVPR, 2015.
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044) , ICML, 2015.
- [Self-critical Sequence Training for Image Captioning](https://arxiv.org/abs/1612.00563) , CVPR, 2017.
- [Attention on Attention for Image Captioning](https://arxiv.org/abs/1908.06954) , ICCV, 2019.
- [Meshed-Memory Transformer for Image Captioning](https://arxiv.org/abs/1912.08226) , CVPR, 2020.
- [Dual-Level Collaborative Transformer for Image Captioning](https://arxiv.org/abs/2101.06462) , AAAI, 2021.
- [Exploiting Cross-Modal Prediction and Relation Consistency for Semi-Supervised Image Captioning](https://arxiv.org/abs/2110.11767) , TCYB, 2022.

## 4 多模态预训练模型

### 4.1 双流Transformer

- [ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/abs/1908.02265) , NIPS, 2019.
- [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) , EMNLP, 2019.
- [ERNIE-ViL: Knowledge Enhanced Vision-Language Representations through Scene Graphs](https://arxiv.org/abs/2006.16934) , AAAI, 2021.
- [UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](https://arxiv.org/abs/2012.15409) , ACL, 2021.

### 4.2 单流Transformer

- [VisualBERT: A simple and performant baseline for vision and language](https://arxiv.org/abs/1908.03557) , arXiv, 2019.
- [Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training](https://arxiv.org/abs/1908.06066) , AAAI, 2020.
- [Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers](https://arxiv.org/abs/2004.00849) , CVPR, 2020.
- [VL-BERT: Pre-training of generic visual-linguistic representations](https://arxiv.org/abs/1908.08530) , ICLR, 2020.
- [UNITER: UNiversal Image-TExt Representation Learning](https://arxiv.org/abs/1909.11740) , ECCV, 2020.
- [Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks](https://arxiv.org/abs/2004.06165) , ECCV, 2020.
- [VinVL: Revisiting Visual Representations in Vision-Language Models](https://arxiv.org/abs/2101.00529) , CVPR, 2021.
- [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) , ICML, 2021.







